{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Docx files to processable txt file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:28.729422Z",
     "end_time": "2023-06-27T12:38:28.746930Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from docx import Document\n",
    "from docx.document import Document as _Document\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.table import _Cell, Table, _Row\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the docx files and creating the"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def iter_block_items(parent):\n",
    "    \"\"\"\n",
    "    Generate a reference to each paragraph and table child within *parent*,\n",
    "    in document order. Each returned value is an instance of either Table or\n",
    "    Paragraph. *parent* would most commonly be a reference to a main\n",
    "    Document object, but also works for a _Cell object, which itself can\n",
    "    contain paragraphs and tables.\n",
    "    \"\"\"\n",
    "    if isinstance(parent, _Document):\n",
    "        parent_elm = parent.element.body\n",
    "    elif isinstance(parent, _Cell):\n",
    "        parent_elm = parent._tc\n",
    "    elif isinstance(parent, _Row):\n",
    "        parent_elm = parent._tr\n",
    "    else:\n",
    "        raise ValueError(\"something's not right\")\n",
    "    for child in parent_elm.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            yield Table(child, parent)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:29.042276Z",
     "end_time": "2023-06-27T12:38:29.071291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:29.281008Z",
     "end_time": "2023-06-27T12:38:29.294769Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text(filename:str):\n",
    "    \"\"\"\n",
    "    Extracts the text and headings from a given Microsoft Word document.\n",
    "\n",
    "    :param filename: name of the file duh\n",
    "    :returns    return_full_text: The full text of the document, including inserted headings for later splits.\n",
    "              like: Heading 1, Heading 2, Heading 3\n",
    "                return_headings: A dictionary with three keys ('Heading 1', 'Heading 2', 'Heading 3')\n",
    "              representing the extracted headings. The values are lists of the corresponding heading texts.\n",
    "    \"\"\"\n",
    "    #Reading the docx file\n",
    "    doc = Document(filename)\n",
    "    full_text = []\n",
    "    return_headings = {\n",
    "        \"Heading 1\": list(),\n",
    "        \"Heading 2\": list(),\n",
    "        \"Heading 3\": list()\n",
    "    }\n",
    "    #block is the yielded object from iter_block_items\n",
    "    for block in iter_block_items(doc):\n",
    "        # We check if what instance is the block if it is paragraph, we append the \"Heading 1\", \"Heading 2\", \"Heading 3\"\n",
    "        # strings, so later we can split by that. For every run we gather the name of the headings, so we can put them into the full_text later\n",
    "        if isinstance(block, Paragraph):\n",
    "            if block.style.name == \"Heading 1\":\n",
    "                full_text.append(\" Heading 1 \" + block.text)\n",
    "                return_headings[\"Heading 1\"].append(block.text)\n",
    "            elif block.style.name == \"Heading 2\":\n",
    "                full_text.append(\" Heading 2 \" + block.text)\n",
    "                return_headings[\"Heading 2\"].append(block.text)\n",
    "            elif block.style.name == \"Heading 3\":\n",
    "                full_text.append(\" Heading 3 \" + block.text)\n",
    "                return_headings[\"Heading 3\"].append(block.text)\n",
    "            else:\n",
    "                full_text.append(block.text)\n",
    "        elif isinstance(block, Table):\n",
    "            for row in block.rows:\n",
    "                row_data = []\n",
    "                for cell in row.cells:\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        row_data.append(paragraph.text)\n",
    "                full_text.append(\"\\t\".join(row_data))\n",
    "\n",
    "    return_full_text = '\\n'.join(full_text)\n",
    "    return return_full_text, return_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:29.501213Z",
     "end_time": "2023-06-27T12:38:30.143197Z"
    }
   },
   "outputs": [],
   "source": [
    "data, headings = zip(*[get_text(os.path.join(\"data\", x)) for x in os.listdir(\"data\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:30.142196Z",
     "end_time": "2023-06-27T12:38:30.159014Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_files_to_txt(source_data):\n",
    "\n",
    "    with open(\"data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in source_data:\n",
    "            f.write(d + \"\\n\")\n",
    "            f.write(\"-------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "write_files_to_txt(source_data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the snippets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def make_smaller_text_original(source_data: tuple, source_headings:dict, smaller_than:int =512, larger_than:int=1000):\n",
    "    \"\"\"\n",
    "    Creating smaller texts for the reader and retriever\n",
    "\n",
    "    :param source_data: the text we read\n",
    "    :param source_headings: every heading by files\n",
    "    :param smaller_than: A custom number where we set a random number to approximate the max length of the texts\n",
    "    :param larger_than: A custom number to determine when to cut the docs into heading 3 snippets\n",
    "    :return: data_dict, file_names: returns the smaller snippets in dict and the filenames which are the keys for the dict\n",
    "    \"\"\"\n",
    "    data_dict = dict()\n",
    "    data_split, file_names = zip(\n",
    "        *[(x.split(\"Heading 1\"), f_name) if \"Heading 1\" in x else (x.split(\"\\n\\n\"), f_name) for x, f_name in\n",
    "          zip(source_data, os.listdir(\"data\"))])\n",
    "    # data_split, file_names = zip(*[(x.split(\"Heading 2\"), f_name) if \"Heading 2\" in x else (x.split(\"Heading 3\"), f_name) if len(x) > 1000 and \"Heading 3\" in x else (x.split(\"\\n\\n\"), f_name) for x, f_name in zip(data, os.listdir(\"data\")) ])\n",
    "\n",
    "    for data_s, file_name, heading in tqdm(zip(data_split, file_names, source_headings)):\n",
    "        data_dict[file_name] = list()\n",
    "        for d in data_s:\n",
    "            #If the length of the current list element is greater than the larger_than, than we splti the text by the \"Heading 3\"\n",
    "            if len(d) > larger_than and \"Heading 2\" in d:\n",
    "                d_split_h2 = d.split(\"Heading 2\")\n",
    "                for d_s in d_split_h2:\n",
    "\n",
    "                    if len(d_s) > larger_than and \"Heading 3\" in d_s:\n",
    "                        d_split_h3 = d.split(\"Heading 3\")\n",
    "                        for d_s_h3 in d_split_h3:\n",
    "                            if      len(d_s_h3) > 200 \\\n",
    "                                    or not data_dict[file_name] \\\n",
    "                                    or d_s_h3 not in heading[\"Heading 1\"] \\\n",
    "                                    or d_s_h3 not in heading[\"Heading 2\"] \\\n",
    "                                    or d_s_h3 not in heading[\"Heading 3\"]:\n",
    "\n",
    "                                data_dict[file_name].append(d_s_h3)\n",
    "                            else:\n",
    "                                data_dict[file_name][-1] += f\"\\n{d_s_h3}\"\n",
    "                    elif      len(d_s) > 200 \\\n",
    "                            or not data_dict[file_name] \\\n",
    "                            or d_s not in heading[\"Heading 1\"] \\\n",
    "                            or d_s not in heading[\"Heading 2\"] \\\n",
    "                            or d_s not in heading[\"Heading 3\"]:\n",
    "\n",
    "                        data_dict[file_name].append(d_s)\n",
    "                    else:\n",
    "                        data_dict[file_name][-1] += f\"\\n{d_s}\"\n",
    "\n",
    "            elif not data_dict[file_name] or len(data_dict[file_name][-1]) >= smaller_than and len(d) >= 100:\n",
    "\n",
    "                data_dict[file_name].append(d)\n",
    "\n",
    "            else:\n",
    "                data_dict[file_name][-1] += f\"\\n{d}\"\n",
    "\n",
    "    return data_dict, file_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:30.178706Z",
     "end_time": "2023-06-27T12:38:30.187715Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:30.384836Z",
     "end_time": "2023-06-27T12:38:30.401976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 15993.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# smaller_data_snipets, fname = make_smaller_text(data, headings=headings)\n",
    "smaller_data_snipets, fname = make_smaller_text_original(data, source_headings=headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inserting headings to every sub snippet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def insert_headings(source_data, source_headings):\n",
    "    \"\"\"\n",
    "    inserts the headings into every sub snippet\n",
    "    :param source_data: dict with filenames as keys and the list of snippets\n",
    "    :param source_headings: previous headings to put into every snippet\n",
    "    :return: the data with headings in it and headings in order, to append them at then and of the paragraph separator line\n",
    "    \"\"\"\n",
    "    heading_return = dict()\n",
    "    for (fn, data_snippet) , heading in zip(source_data.items(), source_headings):\n",
    "        one_i = 0\n",
    "        two_i = 0\n",
    "        three_i = 0\n",
    "        current_heading_one = str()\n",
    "        current_heading_two = str()\n",
    "        current_heading_three = str()\n",
    "        heading_return[fn] = list()\n",
    "        for i, d in enumerate(data_snippet):\n",
    "\n",
    "            for h in heading[\"Heading 1\"]:\n",
    "                if h in d:\n",
    "                    one_i += 1\n",
    "                    current_heading_one = h\n",
    "\n",
    "            for h in heading[\"Heading 2\"]:\n",
    "                if h in d:\n",
    "                    two_i += 1\n",
    "                    current_heading_two = h\n",
    "\n",
    "            for h in heading[\"Heading 3\"]:\n",
    "                if h in d:\n",
    "                    three_i += 1\n",
    "                    current_heading_three = h\n",
    "\n",
    "            if current_heading_three not in d:\n",
    "                source_data[fn][i] = f\"{current_heading_three}\\n\" + source_data[fn][i]\n",
    "\n",
    "            if current_heading_two not in d:\n",
    "                source_data[fn][i] = f\"{current_heading_two}\\n\" + source_data[fn][i]\n",
    "\n",
    "            if current_heading_one not in d:\n",
    "                source_data[fn][i] = f\"{current_heading_one}\\n\" + source_data[fn][i]\n",
    "            heading_return[fn].append(\n",
    "                f\"h1<{current_heading_one}>h2<{current_heading_two}>h3<{current_heading_three}>\\n\")\n",
    "    return source_data, heading_return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:30.663022Z",
     "end_time": "2023-06-27T12:38:30.680202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "smaller_data_snipets_with_headings, headings_ordered = insert_headings(smaller_data_snipets, headings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:31.189210Z",
     "end_time": "2023-06-27T12:38:31.204976Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# for key, value in smaller_data_snipets.items():\n",
    "#     for val_list in value:\n",
    "#         if val_list.__contains__(\"Külföld\"):\n",
    "#             print(value)\n",
    "#             print(\"-----------------------------------------\")\n",
    "#             print(key)\n",
    "#             print(\"-----------------------------------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:32.168307Z",
     "end_time": "2023-06-27T12:38:32.180266Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing the paragraphs to txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:32.518477Z",
     "end_time": "2023-06-27T12:38:32.523483Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_paragraphs_to_txt(source_data, source_headings, txt_name=\"data_paragraphs.txt\"):\n",
    "    \"\"\"\n",
    "    Writes the text into a txt file\n",
    "\n",
    "    :param source_data: text with headings in it\n",
    "    :param source_headings: headings in order, to append them at then and of the paragraph separator line\n",
    "    :param txt_name: preferred name for the txt\n",
    "    \"\"\"\n",
    "    with open(txt_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for (file_name, text), heading_ordered in zip(source_data.items(), source_headings.values()):\n",
    "            val = list(\n",
    "                map(\"\\nparagraphs------------------------------------------------------------------------------------------------------------------------- headings\".join,\n",
    "                    zip(text, heading_ordered)))\n",
    "            val = \"\".join(val)\n",
    "            val = val.replace(\"Heading 3\", \"\")\n",
    "            val = val.replace(\"Heading 2\", \"\")\n",
    "            val = val.replace(\"Heading 1\", \"\")\n",
    "            f.write(val + \"\\n\")\n",
    "            f.write(\n",
    "                f\"file------------------------------------------------------------------------------------------------------------------------- file_name<{file_name}>\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "write_paragraphs_to_txt(smaller_data_snipets_with_headings, headings_ordered)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:33.147239Z",
     "end_time": "2023-06-27T12:38:33.164254Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in smaller_data_snipets_with_headings:\n",
    "    for value in smaller_data_snipets_with_headings[key]:\n",
    "        count += 1\n",
    "        print(key)\n",
    "        print(len(value))\n",
    "count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:33.674099Z",
     "end_time": "2023-06-27T12:38:33.684109Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the txt and then loading it into dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def read_txt_paragraphs(txt_name:str=\"data_paragraphs.txt\" ):\n",
    "    \"\"\"\n",
    "    Reads the txt file, and sorts the data\n",
    "    :param txt_name: preferred name for the txt file\n",
    "    :return: splitted texts, headers and the file names\n",
    "    \"\"\"\n",
    "    with open(txt_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_read = f.read()\n",
    "        # get all file names which are inside <filename>\n",
    "        file_names = re.findall(r\"file_name<([^>]+)\", text_read)\n",
    "        # replace the file names with empty string, so we can split by the file separator line\n",
    "        text_read = re.sub(r\"file_name<([^>]+)>\", \"\", text_read)\n",
    "\n",
    "        splitted_texts_by_file = text_read.split(\n",
    "            \"file------------------------------------------------------------------------------------------------------------------------- \")\n",
    "        splitted_texts_by_paragraph = dict()\n",
    "\n",
    "        for (fn, splitted_text) in zip(file_names, splitted_texts_by_file):\n",
    "            splitted_texts_by_paragraph[fn] = dict()\n",
    "            # splitting the file by the paragraph separator, because there is \\n inbetween last paragraph and the file separator we don't include that\n",
    "            splitted_texts_by_paragraph[fn][\"text\"] = splitted_text.split(\n",
    "                \"paragraphs------------------------------------------------------------------------------------------------------------------------- \")[:-1]\n",
    "            splitted_texts_by_paragraph[fn][\"headers\"] = list()\n",
    "\n",
    "            for i, paragraph in enumerate(splitted_texts_by_paragraph[fn][\"text\"]):\n",
    "                # get all the headers which are inside \"headersh1<headername>h2<headername>h3<headername>\"\n",
    "                splitted_texts_by_paragraph[fn][\"headers\"].append(re.findall(r\"<([^>]+)>\", paragraph))\n",
    "                # replace the headers with empty string, it is not needed in the file\n",
    "                splitted_texts_by_paragraph[fn][\"text\"][i] = re.sub(r\"headingsh1<.*?>h2<.*?>h3<.*?>\\n\", \"\", paragraph)\n",
    "\n",
    "    return splitted_texts_by_paragraph, file_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:35.713860Z",
     "end_time": "2023-06-27T12:38:35.716863Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "para, asd = read_txt_paragraphs(txt_name=\"data_paragraphs.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:36.462293Z",
     "end_time": "2023-06-27T12:38:36.472171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(para).T.reset_index(names=[\"file_names\", \"text\", \"headers\"])\n",
    "df = df.explode(['text', 'headers']).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:36.886970Z",
     "end_time": "2023-06-27T12:38:36.908980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-27T12:38:39.828059Z",
     "end_time": "2023-06-27T12:38:39.846248Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
