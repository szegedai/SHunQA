{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Docx files to processable txt file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from [huspacy github](https://github.com/huspacy/huspacy) download the large model before running the code below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:58:47.543338Z",
     "end_time": "2023-07-19T13:58:48.807560Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from docx import Document\n",
    "from docx.document import Document as _Document\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from docx.table import _Cell, Table, _Row\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "from tqdm import tqdm\n",
    "import huspacy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the docx files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "nlp = huspacy.load()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:58:48.808560Z",
     "end_time": "2023-07-19T13:58:54.683518Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def iter_block_items(parent):\n",
    "    \"\"\"\n",
    "    Generate a reference to each paragraph and table child within *parent*,\n",
    "    in document order. Each returned value is an instance of either Table or\n",
    "    Paragraph. *parent* would most commonly be a reference to a main\n",
    "    Document object, but also works for a _Cell object, which itself can\n",
    "    contain paragraphs and tables.\n",
    "    \"\"\"\n",
    "    if isinstance(parent, _Document):\n",
    "        parent_elm = parent.element.body\n",
    "    elif isinstance(parent, _Cell):\n",
    "        parent_elm = parent._tc\n",
    "    elif isinstance(parent, _Row):\n",
    "        parent_elm = parent._tr\n",
    "    else:\n",
    "        raise ValueError(\"something's not right\")\n",
    "    for child in parent_elm.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            yield Table(child, parent)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:58:54.684519Z",
     "end_time": "2023-07-19T13:58:54.698349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_text(filename: str):\n",
    "    \"\"\"\n",
    "    Extracts the text and headings from a given Microsoft Word document.\n",
    "\n",
    "    :param filename: name of the file duh\n",
    "    :returns    return_full_text: The full text of the document, including inserted headings for later splits.\n",
    "              like: Heading 1, Heading 2, Heading 3\n",
    "                return_headings: A dictionary with three keys ('Heading 1', 'Heading 2', 'Heading 3')\n",
    "              representing the extracted headings. The values are lists of the corresponding heading texts.\n",
    "    \"\"\"\n",
    "    #Reading the docx file\n",
    "    doc = Document(filename)\n",
    "    full_text = []\n",
    "    return_headings = {\n",
    "        \"Heading 1\": list(),\n",
    "        \"Heading 2\": list(),\n",
    "        \"Heading 3\": list()\n",
    "    }\n",
    "    #block is the yielded object from iter_block_items\n",
    "    for block in iter_block_items(doc):\n",
    "        # We check if what instance is the block if it is paragraph, we append the \"Heading 1\", \"Heading 2\", \"Heading 3\"\n",
    "        # strings, so later we can split by that. For every run we gather the name of the headings, so we can put them into the full_text later\n",
    "        if isinstance(block, Paragraph):\n",
    "            if block.style.name == \"Heading 1\" and block.text != \"\":\n",
    "                full_text.append(\" Heading 1 \" + block.text)\n",
    "                return_headings[\"Heading 1\"].append(\" Heading 1 \" + block.text)\n",
    "            elif block.style.name == \"Heading 2\" and block.text != \"\":\n",
    "                full_text.append(\" Heading 2 \" + block.text)\n",
    "                return_headings[\"Heading 2\"].append(\" Heading 2 \" + block.text)\n",
    "            elif block.style.name == \"Heading 3\" and block.text != \"\":\n",
    "                full_text.append(\" Heading 3 \" + block.text)\n",
    "                return_headings[\"Heading 3\"].append(\" Heading 3 \" + block.text)\n",
    "            else:\n",
    "                full_text.append(block.text)\n",
    "        elif isinstance(block, Table):\n",
    "            for row in block.rows:\n",
    "                row_data = []\n",
    "                for cell in row.cells:\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        row_data.append(paragraph.text)\n",
    "                full_text.append(\"\\t\".join(row_data))\n",
    "\n",
    "    return_full_text = '\\n'.join(full_text)\n",
    "    return return_full_text, return_headings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:58:54.719367Z",
     "end_time": "2023-07-19T13:58:54.730379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"..\", \"data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:58:54.747393Z",
     "end_time": "2023-07-19T13:58:54.762408Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "data, headings = zip(*[get_text(os.path.join(data_path, x)) for x in os.listdir(data_path)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:58:54.764409Z",
     "end_time": "2023-07-19T13:58:55.432321Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the snippets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_smaller_text(source_data: tuple, source_headings: dict, smaller_than: int = 128,\n",
    "                               larger_than: int = 384, path: str = \"../data\"):\n",
    "    \"\"\"\n",
    "    Creating smaller texts for the reader and retriever\n",
    "\n",
    "    :param source_data: the text we read\n",
    "    :param source_headings: every heading by files\n",
    "    :param smaller_than: A custom number where we set a random number to approximate the max length of the texts\n",
    "    :param larger_than: A custom number to determine when to cut the docs into heading 3 snippets\n",
    "    :param path: Path of the data\n",
    "    :return: data_dict, file_names: returns the smaller snippets in dict and the filenames which are the keys for the dict\n",
    "    \"\"\"\n",
    "    data_dict = dict()\n",
    "    data_split, file_names = zip(*[([\" Heading 1 \" + y.lstrip() for y in x.split(\"Heading 1\")], f_name) if \"Heading 1\" in x else (x.split(\"\\n\\n\"), f_name) for x, f_name in zip(source_data, os.listdir(path))])\n",
    "    # data_split, file_names = zip(*[(x.split(\"Heading 2\"), f_name) if \"Heading 2\" in x else (x.split(\"Heading 3\"), f_name) if len(x) > 1000 and \"Heading 3\" in x else (x.split(\"\\n\\n\"), f_name) for x, f_name in zip(data, os.listdir(\"data\")) ])\n",
    "\n",
    "    for data_s, file_name, heading in tqdm(zip(data_split, file_names, source_headings)):\n",
    "        data_dict[file_name] = list()\n",
    "\n",
    "        for d in data_s:\n",
    "\n",
    "            #If the length of the current list element is greater than the larger_than, than we split the text by the \"Heading 2\"\n",
    "            if len(d) > larger_than and \"Heading 2\" in d:\n",
    "\n",
    "                d_split_h2 = d.split(\" Heading 2 \")\n",
    "                d_split_h2 = [\" Heading 2 \" + x for x in d_split_h2 if x]\n",
    "                d_split_h2[0] =  d_split_h2[0].replace(\" Heading 2 \", \"\")\n",
    "\n",
    "                for d_s in d_split_h2:\n",
    "                    #If the length of the current list element is greater than the larger_than and there is Heading 3 substring in the split, than we split the text by the \"Heading 3\"\n",
    "                    if len(d_s) > larger_than and \"Heading 3\" in d_s:\n",
    "\n",
    "                        d_split_h3 = d_s.split(\" Heading 3 \")\n",
    "                        d_split_h3 = [\" Heading 3 \" + x for x in d_split_h3 if x]\n",
    "                        d_split_h3[0] =  d_split_h3[0].replace(\" Heading 3 \", \"\")\n",
    "\n",
    "                        for d_s_h3 in d_split_h3:\n",
    "                            #If the length of the current list element is greater than the larger_than, than we split the text with huspacy\n",
    "                            if len(d_s_h3) > larger_than:\n",
    "\n",
    "                                d_sentences = nlp(d_s_h3)\n",
    "\n",
    "                                for d_sentence in d_sentences.sents:\n",
    "\n",
    "                                    if not data_dict[file_name] or len(d_sentence) + len(\n",
    "                                            data_dict[file_name][-1]) > larger_than:\n",
    "\n",
    "                                        data_dict[file_name].append(d_sentence.text)\n",
    "                                        continue\n",
    "                                    data_dict[file_name][-1] += f\"\\n{d_sentence.text}\"\n",
    "                                continue\n",
    "\n",
    "\n",
    "                            #If the length of the current list element is greater than the lists last element length + the current element, than we split the text with huspacy\n",
    "                            elif len(d_s_h3) + len(data_dict[file_name][-1]) > larger_than \\\n",
    "                                    or not data_dict[file_name]:\n",
    "\n",
    "                                d_sentences = nlp(d_s_h3)\n",
    "\n",
    "                                for d_sentence in d_sentences.sents:\n",
    "\n",
    "                                    if not data_dict[file_name] or len(d_sentence) + len(\n",
    "                                            data_dict[file_name][-1]) > larger_than:\n",
    "                                        data_dict[file_name].append(d_sentence.text)\n",
    "\n",
    "                                        continue\n",
    "                                    data_dict[file_name][-1] += f\"\\n{d_sentence.text}\"\n",
    "                                continue\n",
    "\n",
    "\n",
    "\n",
    "                            data_dict[file_name][-1] += f\"\\n{d_s_h3}\"\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    #If the length of the current list element is greater than the larger_than, than we split the text with huspacy\n",
    "                    elif len(d_s) > larger_than \\\n",
    "                        or not data_dict[file_name]:\n",
    "\n",
    "                        d_sentences = nlp(d_s)\n",
    "\n",
    "                        for d_sentence in d_sentences.sents:\n",
    "\n",
    "                            if not data_dict[file_name] or len(d_sentence) + len(\n",
    "                                    data_dict[file_name][-1]) > larger_than:\n",
    "                                data_dict[file_name].append(d_sentence.text)\n",
    "                                continue\n",
    "                            data_dict[file_name][-1] += f\"\\n{d_sentence.text}\"\n",
    "                        continue\n",
    "\n",
    "                    data_dict[file_name][-1] += f\"\\n{d_s}\"\n",
    "                continue\n",
    "\n",
    "\n",
    "            #If the length of the current list element is greater than the larger_than, than we split the text with huspacy\n",
    "            elif len(d) > larger_than:\n",
    "                d_sentences = nlp(d)\n",
    "\n",
    "                for d_sentence in d_sentences.sents:\n",
    "\n",
    "                    if not data_dict[file_name] or len(d_sentence) + len(data_dict[file_name][-1]) > larger_than:\n",
    "                        data_dict[file_name].append(d_sentence.text)\n",
    "                        continue\n",
    "                    data_dict[file_name][-1] += f\"\\n{d_sentence.text}\"\n",
    "                continue\n",
    "\n",
    "\n",
    "            elif not data_dict[file_name] or len(data_dict[file_name][-1]) >= smaller_than and len(d) >= 100:\n",
    "                data_dict[file_name].append(d)\n",
    "                continue\n",
    "\n",
    "            data_dict[file_name][-1] += f\"\\n{d}\"\n",
    "\n",
    "    return data_dict, file_names\n",
    "\n",
    "\n",
    "smaller_data_snipets, fname = make_smaller_text(data, source_headings=headings, path=data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:59:01.872803Z",
     "end_time": "2023-07-19T13:59:17.501443Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inserting headings to every sub snippet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def insert_headings(source_data, source_headings):\n",
    "    \"\"\"\n",
    "    inserts the headings into every sub snippet\n",
    "    :param source_data: dict with filenames as keys and the list of snippets\n",
    "    :param source_headings: previous headings to put into every snippet\n",
    "    :return: the data with headings in it and headings in order, to append them at then and of the paragraph separator line\n",
    "    \"\"\"\n",
    "    heading_return = dict()\n",
    "    for (fn, data_snippet), heading in zip(source_data.items(), source_headings):\n",
    "        one_i = 0\n",
    "        two_i = 0\n",
    "        three_i = 0\n",
    "        current_heading_one = str()\n",
    "        current_heading_two = str()\n",
    "        current_heading_three = str()\n",
    "        heading_return[fn] = list()\n",
    "        for i, d in enumerate(data_snippet):\n",
    "\n",
    "            for h in heading[\"Heading 1\"]:\n",
    "                if h in d:\n",
    "                    one_i += 1\n",
    "                    if h != current_heading_one:\n",
    "                        current_heading_one = h\n",
    "                        current_heading_two = \"\"\n",
    "                        current_heading_three = \"\"\n",
    "                    else:\n",
    "                        current_heading_one = h\n",
    "\n",
    "            for h in heading[\"Heading 2\"]:\n",
    "                if h in d:\n",
    "                    two_i += 1\n",
    "                    if h != current_heading_two:\n",
    "                        current_heading_two = h\n",
    "                        current_heading_three = \"\"\n",
    "                    else:\n",
    "                        current_heading_two = h\n",
    "\n",
    "            for h in heading[\"Heading 3\"]:\n",
    "                if h in d:\n",
    "                    three_i += 1\n",
    "                    current_heading_three = h\n",
    "            # Here you will see the problem, its probably\n",
    "            # print(\"=\" * 50)\n",
    "            # print(fn)\n",
    "            # print(current_heading_one)\n",
    "            # print(current_heading_two)\n",
    "            # print(current_heading_three)\n",
    "            # print(\"=\" * 50)\n",
    "            if current_heading_three not in d:\n",
    "                source_data[fn][i] = f\"{current_heading_three}\\n\" + source_data[fn][i]\n",
    "\n",
    "            if current_heading_two not in d:\n",
    "                source_data[fn][i] = f\"{current_heading_two}\\n\" + source_data[fn][i]\n",
    "\n",
    "            if current_heading_one not in d:\n",
    "                source_data[fn][i] = f\"{current_heading_one}\\n\" + source_data[fn][i]\n",
    "            heading_return[fn].append(\n",
    "                f\"h1<{current_heading_one}>h2<{current_heading_two}>h3<{current_heading_three}>\\n\")\n",
    "            # print(fn, f\"h1<{current_heading_one}>h2<{current_heading_two}>h3<{current_heading_three}>\\n\")\n",
    "    return source_data, heading_return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:59:17.507448Z",
     "end_time": "2023-07-19T13:59:17.522114Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smaller_data_snipets_with_headings, headings_ordered = insert_headings(smaller_data_snipets, headings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:59:17.516870Z",
     "end_time": "2023-07-19T13:59:17.535798Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writing the paragraphs to txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def write_paragraphs_to_txt(source_data, source_headings, txt_name=\"data_paragraphs.txt\"):\n",
    "    \"\"\"\n",
    "    Writes the text into a txt file\n",
    "\n",
    "    :param source_data: text with headings in it\n",
    "    :param source_headings: headings in order, to append them at then and of the paragraph separator line\n",
    "    :param txt_name: preferred name for the txt\n",
    "    \"\"\"\n",
    "    with open(txt_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for (file_name, text), heading_ordered in zip(source_data.items(), source_headings.values()):\n",
    "            val = list(\n",
    "                map(\"\\nparagraphs------------------------------------------------------------------------------------------------------------------------- headings\".join, zip(text, heading_ordered)))\n",
    "            val = \"\".join(val)\n",
    "            val = val.replace(\" Heading 3 \", \"\")\n",
    "            val = val.replace(\" Heading 2 \", \"\")\n",
    "            val = val.replace(\" Heading 1 \", \"\")\n",
    "            val = val.replace(\" Heading 1\", \"\")\n",
    "            f.write(val + \"\\n\")\n",
    "            f.write(\n",
    "                f\"file------------------------------------------------------------------------------------------------------------------------- file_name<{file_name}>\\n\")\n",
    "\n",
    "\n",
    "write_paragraphs_to_txt(smaller_data_snipets_with_headings, headings_ordered)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-19T13:59:21.967696Z",
     "end_time": "2023-07-19T13:59:21.989519Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading the txt and then loading it into dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def read_txt_paragraphs(txt_name: str = \"data_paragraphs.txt\"):\n",
    "    \"\"\"\n",
    "    Reads the txt file, and sorts the data\n",
    "    :param txt_name: preferred name for the txt file\n",
    "    :return splitted_texts_by_paragraph, file_names: splitted texts, headers and the file names\n",
    "    \"\"\"\n",
    "    with open(txt_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_read = f.read()\n",
    "        # get all file names which are inside <filename>\n",
    "        file_names = re.findall(r\"file_name<([^>]+)\", text_read)\n",
    "        # replace the file names with empty string, so we can split by the file separator line\n",
    "        text_read = re.sub(r\"file_name<([^>]+)>\", \"\", text_read)\n",
    "\n",
    "        splitted_texts_by_file = text_read.split(\n",
    "            \"file------------------------------------------------------------------------------------------------------------------------- \")\n",
    "        splitted_texts_by_paragraph = dict()\n",
    "\n",
    "        for (fn, splitted_text) in zip(file_names, splitted_texts_by_file):\n",
    "            splitted_texts_by_paragraph[fn] = dict()\n",
    "            # splitting the file by the paragraph separator, because there is \\n inbetween last paragraph and the file separator we don't include that\n",
    "            splitted_texts_by_paragraph[fn][\"text\"] = splitted_text.split(\n",
    "                \"paragraphs------------------------------------------------------------------------------------------------------------------------- \")[\n",
    "                                                      :-1]\n",
    "            splitted_texts_by_paragraph[fn][\"headers\"] = list()\n",
    "\n",
    "            for i, paragraph in enumerate(splitted_texts_by_paragraph[fn][\"text\"]):\n",
    "                # get all the headers which are inside \"headersh1<headername>h2<headername>h3<headername>\"\n",
    "                splitted_texts_by_paragraph[fn][\"headers\"].append(re.findall(r\"<([^>]+)>\", paragraph))\n",
    "                # replace the headers with empty string, it is not needed in the file\n",
    "                splitted_texts_by_paragraph[fn][\"text\"][i] = re.sub(r\"headingsh1<.*?>h2<.*?>h3<.*?>\\n\", \"\", paragraph)\n",
    "\n",
    "    return splitted_texts_by_paragraph, file_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "para, asd = read_txt_paragraphs(txt_name=\"data_paragraphs.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(para).T.reset_index(names=[\"file_names\", \"text\", \"headers\"])\n",
    "df = df.explode(['text', 'headers']).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}